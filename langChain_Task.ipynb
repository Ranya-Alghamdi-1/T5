{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranya-Alghamdi-1/T5/blob/main/langChain_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fbb33b",
      "metadata": {
        "id": "45fbb33b"
      },
      "source": [
        "# LangChain Tutorial Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a0df42",
      "metadata": {
        "id": "34a0df42"
      },
      "source": [
        "\n",
        "In this notebook, you will practice using LangChain to interact with large language models (LLMs),\n",
        "build chains, agents, and utilize memory. Fill in the code blocks with your implementations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169dd23c",
      "metadata": {
        "id": "169dd23c"
      },
      "source": [
        "## Exercise 1: Basic LLM Query"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf03f4a7",
      "metadata": {
        "id": "bf03f4a7"
      },
      "source": [
        "In this exercise, you will set up a basic interaction with the GROQ LLaMA model using LangChain.\n",
        "\n",
        "1. Initialize the LLM (Use GROQ and chose LLM).\n",
        "2. Create a prompt that asks the LLM to generate a story about a topic.\n",
        "3. Run the LLM chain to retrieve the response.\n",
        "\n",
        "**Steps**:\n",
        "- Import required modules from `langchain`.\n",
        "- Initialize the LLM with your GROQ API key.\n",
        "- Create a prompt template that takes a topic as input.\n",
        "- Create an LLM Chain and run it to get a response.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-groq duckduckgo-search geopy requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrAHltgT3SPf",
        "outputId": "d3808641-8cb4-4a8d-d8fa-17865f30253e",
        "collapsed": true
      },
      "id": "IrAHltgT3SPf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-6.2.11-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Collecting primp>=0.6.1 (from duckduckgo-search)\n",
            "  Downloading primp-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading duckduckgo_search-6.2.11-py3-none-any.whl (27 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.121-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, primp, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, duckduckgo-search, pydantic-settings, httpx, dataclasses-json, langsmith, groq, langchain-core, langchain-text-splitters, langchain-groq, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 duckduckgo-search-6.2.11 groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.0 langchain-groq-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.121 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 primp-0.6.2 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"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\""
      ],
      "metadata": {
        "id": "k1MN2B5Insim"
      },
      "id": "k1MN2B5Insim",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# A prompt template that takes a topic as input\n",
        "prompt_template = PromptTemplate.from_template(\"Write a short story about {topic}.\")\n",
        "\n",
        "# Instantiate the ChatGroq language model\n",
        "llm = ChatGroq(groq_api_key=groq_api_key) # Make sure groq_api_key is defined\n",
        "\n",
        "# Create an LLM Chain and run it to get a response\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Run the LLM chain with a topic\n",
        "topic = \"a heroic dog\"\n",
        "response = llm_chain.run(topic)\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAv98lg9mFIl",
        "outputId": "c5b622e8-8244-4725-9141-8dff63420f18"
      },
      "id": "AAv98lg9mFIl",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-aa69c0c19f0a>:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "<ipython-input-10-aa69c0c19f0a>:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  response = llm_chain.run(topic)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in the quaint little town of Meadowgrove, there lived an extraordinary dog named Max. Max was not your ordinary dog; he was a large, fluffy Newfoundland with a heart as big as his size.\n",
            "\n",
            "One sunny afternoon, while Max was taking his usual stroll around the neighborhood, he noticed something unusual. A small kitten was stuck at the top of a tall tree, meowing frantically for help. The little creature was too scared to climb down, and its mother was pacing anxiously at the base of the tree.\n",
            "\n",
            "Without wasting a moment, Max sprang into action. He ran towards the house where the family owned a ladder. After several attempts to balance the ladder against the tree, Max finally managed to secure it. With one swift movement, he climbed up the ladder, inching closer to the frightened kitten.\n",
            "\n",
            "As Max approached the kitten, it recoiled in fear, but Max gently nudged it with his nose, assuring it that he meant no harm. The kitten slowly calmed down and allowed Max to pick it up gently in his mouth. Carefully, he descended the ladder, ensuring the kitten's safety.\n",
            "\n",
            "Once on solid ground, Max placed the kitten near its mother. The overjoyed feline mother immediately began licking her baby, filled with relief. The family who owned the kitten rushed out, thanking Max profusely for his bravery and kindness.\n",
            "\n",
            "From that day forward, Max became a local hero in Meadowgrove. His story of courage and compassion spread throughout the town, inspiring everyone he met. And though Max remained humble about his heroic act, his chest would swell with pride whenever someone called him a hero. After all, he was just doing what he believed every living being should do - help those in need.\n",
            "\n",
            "And so, life in Meadowgrove continued, filled with love and admiration for the heroic Newfoundland named Max. His tale served as a reminder that even the smallest act of kindness can make a significant difference in someone's life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff1b3fa",
      "metadata": {
        "id": "1ff1b3fa"
      },
      "source": [
        "## Exercise 2: Building a Conversational Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6e3e8e2",
      "metadata": {
        "id": "a6e3e8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fa9b12-0007-42e8-8e71-64d3de36bbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-86b491d58bdf>:18: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  agent = initialize_agent(llm=llm, tools=tools, agent_type=\"zero-shot\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Al Baha city is located in the Al Baha region of Saudi Arabia and is known for its lush valleys, mountains, and agricultural terraces. The city has a rich history and a mild climate, making it a popular tourist destination.\n"
          ]
        }
      ],
      "source": [
        "def search_tool(query):\n",
        "    # Mock search function\n",
        "    return f\"Search results for {query}\"\n",
        "\n",
        "# Initialize the agent using the tool and the LLM\n",
        "from langchain.agents import Tool, initialize_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "search = Tool(\n",
        "    name=\"search\",\n",
        "    func=search_tool,\n",
        "    description=\"Performs a search query\"\n",
        ")\n",
        "\n",
        "tools = [search]\n",
        "\n",
        "# Use GroqLLM for decision-making (using LLM for the conversational agent)\n",
        "agent = initialize_agent(llm=llm, tools=tools, agent_type=\"zero-shot\")\n",
        "\n",
        "#Run the agent with sample inputs\n",
        "response = agent.run(\"Find me information about the Al Baha city\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6a1f26",
      "metadata": {
        "id": "4a6a1f26"
      },
      "source": [
        "\n",
        "In this exercise, you will create a conversational agent that can interact with a user, make decisions,\n",
        "and use external tools like a search tool.\n",
        "\n",
        "1. Define a tool.\n",
        "2. Create an agent that can decide whether to use the tool or interact with the LLM.\n",
        "3. Run the agent with various inputs.\n",
        "\n",
        "**Steps**:\n",
        "- Define the search tool using a function.\n",
        "- Initialize an agent using the tool and the LLM.\n",
        "- Run the agent with sample inputs.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf94d2f",
      "metadata": {
        "id": "3cf94d2f"
      },
      "source": [
        "## Exercise 3: Using LLM as Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ff48d9",
      "metadata": {
        "id": "53ff48d9"
      },
      "source": [
        "\n",
        "In this exercise, you will use an LLM to summarize and retain information from conversations.\n",
        "\n",
        "1. Set up LLM-based memory.\n",
        "2. Create a conversation with the LLM and memory.\n",
        "3. Ask follow-up questions using memory to retrieve past context.\n",
        "\n",
        "**Steps**:\n",
        "- Initialize summarization-based memory.\n",
        "- Run a few queries and retrieve responses.\n",
        "- Ask follow-up questions that reference previous interactions.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import memory class and initialize summarization-based memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"], template=template\n",
        ")\n",
        "\n",
        "#Create a conversational agent with memory\n",
        "from langchain.chains import LLMChain\n",
        "llm_chain_with_memory = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "#Run a few queries and retrieve responses\n",
        "response_1 = llm_chain_with_memory.run(\"Tell me about the history of KSA.\")\n",
        "print(response_1)\n",
        "\n",
        "response_2 = llm_chain_with_memory.run(\"Can you summarize what we just talked about?\")\n",
        "print(response_2)\n",
        "\n",
        "#  Ask follow-up questions that reference previous interactions\n",
        "response_3 = llm_chain_with_memory.run(\"Can you expand on the role of KSA in Riyadh season?\")\n",
        "print(response_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE2nZBPFoibm",
        "outputId": "b8553258-fab6-404a-acad-e4e7f6ad1a2d"
      },
      "id": "wE2nZBPFoibm",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to provide a brief overview of the history of the Kingdom of Saudi Arabia (KSA).\n",
            "\n",
            "The history of Saudi Arabia is deeply rooted in the history of Islam, as it is the birthplace of the religion. The two holiest cities in Islam, Mecca and Medina, are located in Saudi Arabia.\n",
            "\n",
            "In the 18th century, the Saudi Arabian state, as it exists today, began to take shape under the leadership of Muhammad ibn Saud, the founder of the First Saudi State. The Saudi state was built on a combination of religious and political ideology based on Islam, specifically the strict Wahhabi interpretation of it.\n",
            "\n",
            "The First Saudi State was established in 1744 and lasted until 1818 when it was defeated by the Ottoman Empire. However, the Saudi state was reestablished in 1824 as the Second Saudi State, which lasted until 1891.\n",
            "\n",
            "The modern Kingdom of Saudi Arabia was founded in 1932 by Abdulaziz Ibn Saud, who had succeeded in unifying most of the Arabian Peninsula under his rule. He established a centralized state that was heavily influenced by Wahhabi Islam.\n",
            "\n",
            "The discovery of oil in Saudi Arabia in the 1930s transformed the country economically and politically. It led to a massive increase in wealth and influence, and Saudi Arabia became a key player in the global oil market.\n",
            "\n",
            "Today, Saudi Arabia is a constitutional monarchy, with the King acting as the head of state and government. The country faces a number of challenges, including economic diversification, human rights concerns, and regional geopolitical issues.\n",
            "\n",
            "This is a very brief and simplified summary of a complex and rich history. If you're interested in a specific period or aspect of Saudi Arabian history, I'd be happy to try and provide more information.\n",
            "Absolutely! The history of Saudi Arabia is predominantly tied to the history of Islam, with the two holiest cities, Mecca and Medina, located within its borders. The modern Saudi state began to take shape in the 18th century under Muhammad ibn Saud, the founder of the First Saudi State. After a series of events, the Kingdom of Saudi Arabia was officially established in 1932 by Abdulaziz Ibn Saud. The discovery of oil in the 1930s brought significant economic and political changes, elevating Saudi Arabia's global influence and wealth. Today, it's a constitutional monarchy facing challenges such as economic diversification, human rights concerns, and regional geopolitical issues.\n",
            "Of course! The Riyadh Season is a large-scale cultural and entertainment event held in the Saudi Arabian capital, Riyadh. It's part of the Saudi Arabian Vision 2030, an ambitious plan to diversify the country's economy and develop its public sectors, including entertainment, tourism, and culture.\n",
            "\n",
            "The Kingdom of Saudi Arabia (KSA) plays a significant role in the Riyadh Season, as it's an initiative driven by the government to enhance the quality of life for its citizens and promote the country as a global tourist destination. The Saudi Arabian General Entertainment Authority (GEA) is responsible for organizing these events, which include a variety of activities such as concerts, sports events, cultural festivals, and family-oriented entertainment.\n",
            "\n",
            "These events contribute to the KSA's goal of creating a vibrant and inclusive society, showcasing its rich heritage, and providing opportunities for local and international talent to participate. Moreover, the Riyadh Season reflects the country's desire to modernize and open up to the world while preserving its cultural and religious values. The KSA aims to position itself as a hub for entertainment and culture in the region and beyond.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630fe2fb",
      "metadata": {
        "id": "630fe2fb"
      },
      "source": [
        "## Exercise 4: Combining Tools and Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17dba76c",
      "metadata": {
        "id": "17dba76c"
      },
      "source": [
        "\n",
        "In this final exercise, you will build an intelligent agent that can use both tools (like an API) and memory.\n",
        "\n",
        "1. Define an external tool (like a weather API).\n",
        "2. Set up an agent that uses both the tool and LLM memory.\n",
        "3. Interact with the agent, combining memory and external data.\n",
        "\n",
        "**Steps**:\n",
        "- Define a weather API tool (mock or real API).\n",
        "- Initialize the agent with memory and the tool.\n",
        "- Run the agent with inputs, and check how it uses both memory and tools.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5e11ad68",
      "metadata": {
        "id": "5e11ad68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5322008-a838-44e4-e74a-cd6dbc57eac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Al Baha is sunny with a temperature of 25°C.\n",
            "We have discussed various weather conditions in different locations. I have provided information about temperatures, precipitation, and weather events. We may have also compared weather in different places and considered the impact of weather on daily activities.\n",
            "The weather in Riyadh is sunny with a temperature of 25°C.\n"
          ]
        }
      ],
      "source": [
        "# Define a mock weather API tool as a function\n",
        "def mock_weather_api(location):\n",
        "    return f\"The weather in {location} is sunny with a temperature of 25°C.\"\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Define a weather tool using the mock API\n",
        "weather_tool = Tool(\n",
        "    name=\"Weather API\",\n",
        "    func=mock_weather_api,\n",
        "    description=\"Provides weather information for a given location.\"\n",
        ")\n",
        "\n",
        "# Create a prompt template that includes history and human input\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"],\n",
        "    template=\"\"\"\n",
        "    {history}\n",
        "    Human: {human_input}\n",
        "    Assistant:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "#  Set up an agent that uses both memory and the weather tool\n",
        "tools = [weather_tool]\n",
        "\n",
        "agent_with_memory_and_tool = initialize_agent(\n",
        "    llm=llm,   # Use the LLM directly, not LLMChain\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    agent_type=\"zero-shot\"  # Adjust agent type if needed\n",
        ")\n",
        "\n",
        "# Run the agent with inputs and check how it uses both memory and tools\n",
        "# Ask for the weather in Al Baha\n",
        "response_1 = agent_with_memory_and_tool.run(\"What's the weather in Al Baha?\")\n",
        "print(response_1)\n",
        "\n",
        "# Ask the agent to summarize the conversation so far\n",
        "response_2 = agent_with_memory_and_tool.run(\"Can you summarize what we've discussed so far?\")\n",
        "print(response_2)\n",
        "\n",
        "# Ask for the weather in Riyadh\n",
        "response_3 = agent_with_memory_and_tool.run(\"What's the weather in Riyadh?\")\n",
        "print(response_3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkoSazM5sA6o"
      },
      "id": "dkoSazM5sA6o",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}